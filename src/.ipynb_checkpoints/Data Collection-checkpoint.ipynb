{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import uuid\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in image url data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using the google open image dataset v4, you can download the aggregated image info from here: https://storage.googleapis.com/openimages/web/download.html .  \n",
    "I need more file:\n",
    "    * The file which contains the ImageID and the urls where we can download the real images, this will be the image_id_url DataFrame. \n",
    "    * A class description file, which is a switch table between Image label names and their identifiers.  \n",
    "    * The image label file, where ImageID and LabelId connected. It is separated in each dataset (train,valid,test).  \n",
    "First load up into the memory the csv-s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_id_url = pd.read_csv('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\image_ids_and_rotation.csv',engine='python',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_id_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_labels = pd.read_csv('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\train-annotations-human-imagelabels.csv',engine='python',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_validation_labels = pd.read_csv('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\validation-annotations-human-imagelabels.csv',engine='python',sep = ',')\n",
    "image_test_labels = pd.read_csv('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\test-annotations-human-imagelabels.csv',engine='python',sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the data. From the website you can download one-by-one, but i don't need it separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_labels = pd.concat( [image_labels,image_validation_labels, image_test_labels],ignore_index=True )\n",
    "image_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_description = pd.read_csv('O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\class-descriptions.csv',engine='python',header = None, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to choose those files which contains cities. I find its label id and save into (city_label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_label = class_description[class_description[1] == 'Skyline'][0]\n",
    "city_label = city_label.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that i collect all image id, what label equal to city_label. Confidence is important beause there are pictures in the dataset which label is city, but doesn't contains one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_id_df = image_labels.loc[(image_labels['LabelName'] == city_label) & (image_labels['Confidence'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I inner join the collected city_id_df with the biggest table: image_id_url (this contains urls and author informations) on ImageID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = image_id_url.merge(city_id_df, on='ImageID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to use the smallest size of the picture, i don't need the best quality now. I'm filter those rows where the smallest image url column is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = merged_df.loc[pd.notnull(merged_df['Thumbnail300KURL'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example data from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(image_urls['Thumbnail300KURL'][11])\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if are there any picture which not jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_jpg_df = image_urls.loc[image_urls['Thumbnail300KURL'].str.rsplit('.',1).apply(lambda x: 'jpg' not in x[1])]\n",
    "list(not_jpg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the Dataframe header row, so there are only jpg url page. The bad reality is that flickr response back a few time an blank image whith \"Image no longer exists\" with HTTP status code 200, luckily its Content-Type is image/png so i can filter these pictures out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_folder = 'O:/ProgrammingSoftwares/anaconda_projects/dp_nagyhazi/data/images/'\n",
    "data_handler = DataCollector()\n",
    "data_handler.load_datas(image_id_rotation_csv, train_annotation_csv, validation_annotation_csv, test_annotation_csv, class_desc_csv)\n",
    "#data_handler.collect_small_images(image_urls, image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_by_labelName('Skylines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_handler.collect_small_images(data_handler.result_label_df , image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id_rotation_csv = 'O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\image_ids_and_rotation.csv'\n",
    "train_annotation_csv = 'O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\train-annotations-human-imagelabels.csv'\n",
    "validation_annotation_csv = 'O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\validation-annotations-human-imagelabels.csv'\n",
    "test_annotation_csv = 'O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\test-annotations-human-imagelabels.csv'\n",
    "class_desc_csv = 'O:\\\\ProgrammingSoftwares\\\\anaconda_projects\\\\dp_nagyhazi\\\\data\\\\class-descriptions.csv'\n",
    "\n",
    "class DataCollector:\n",
    "    \n",
    "\n",
    "    def load_datas(self, image_id_url_csv, train_annotation_csv, validation_annotation_csv, test_annotation_csv, class_desc_csv):\n",
    "        self.image_id_url = pd.read_csv(image_id_url_csv,engine='python', sep = ',')\n",
    "\n",
    "        self.image_train_labels = pd.read_csv(train_annotation_csv,engine='python', sep = ',')\n",
    "        self.image_validation_labels = pd.read_csv(validation_annotation_csv, engine='python',sep = ',')\n",
    "        self.image_test_labels = pd.read_csv(test_annotation_csv,engine='python',sep = ',')\n",
    "\n",
    "        self.class_description = pd.read_csv(class_desc_csv,engine='python',header = None, sep = ',')\n",
    "\n",
    "        self.image_labels = pd.concat( [self.image_train_labels,self.image_validation_labels, self.image_test_labels] )\n",
    "    \n",
    "    def find_by_labelName(self, label_name):\n",
    "        label_id = self.class_description[self.class_description[1] == label_name][0]\n",
    "        label_id = label_id.values[0]\n",
    "        \n",
    "        labeled_df = self.image_labels.loc[(self.image_labels['LabelName'] == label_id) & (self.image_labels['Confidence'] == 1)]\n",
    "        merged_df = self.image_id_url.merge(labeled_df, on='ImageID', how='inner')\n",
    "        \n",
    "        self.result_label_df = merged_df.loc[pd.notnull(merged_df['Thumbnail300KURL'])]\n",
    "        return self.result_label_df\n",
    "        \n",
    "    \n",
    "    '''Long running iteration!!\n",
    "    Iterate through row-by-row, download and save image. If the Http header Content-Type == image/png\n",
    "    then it won't download the image. This is because blank image comes with \"Image no longer exists\", HTTP status code 200.\n",
    "        Parameters: \n",
    "            - image_urls: Pandas DataFrame cleaned url links, skip if url link is None.\n",
    "            - folder: The folder where do you want to download the images.\n",
    "    '''\n",
    "    def collect_small_images(self,image_urls,folder):\n",
    "        for index, row in image_urls.iterrows():\n",
    "\n",
    "            if not pd.notnull(row['Thumbnail300KURL']):\n",
    "                return\n",
    "            url = row['Thumbnail300KURL']\n",
    "            filename = folder + str(uuid.uuid1())+ '.jpg'\n",
    "            # Checking png is important, because there are pictures which is no longer exist, therefore im getting back a picture\n",
    "            # with http status code 200 but it is a blank pic.\n",
    "            if 'image/png' not in urllib.request.urlopen(url).info()['Content-Type']:\n",
    "                urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
